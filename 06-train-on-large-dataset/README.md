# TensorFlow 2 Tutorial 6: Train on Large Dataset

This tutorial explains how to handle large datasets for training in TensorFlow 2.

If you have a large dataset (i.e., tens of GB data), they likely cannot fit your memory. In that case, it is better to design data pipeline to read data on the fly instead of saving the whole data in the memory.

The one way of the handling large dataset is using ```Model.fit_generator``` instead of ```Model.fit```. In that case, the data is created by a Generator when it is demanded during the training phase. [Original documentation](https://keras.io/models/sequential/#fit_generator) says that *the generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.* The point in here is that data augmentation on images on CPU may be optimized if the augmentation process is moved on the GPU, as well.

For efficient data reading and augmentation for large dataset, we will use ```tf.data.Dataset``` object. It allows us to apply custom mapping functions (e.g., for data preprocessing), splitting data into batches, shuffling etc. Although there are several ways of creating ```tf.data.Dataset``` object (such as from numpy arrays  using ```from_tensor_slices```), we look into creating a dataset from a generator. So we can create training or testing data on the fly and process it in GPU. 
  

## Reproduction

```

python train_on_large_dataset.py

```

  
  

## Generators to Handle Large Datasets

First, we define the generator for training data:

```
def  train_generator():

  	indices = np.arange(len(train_images_path))
  	np.random.shuffle(indices)
  
  	for i in indices:

		X = np.array(resize(imread(image_path),  (HEIGHT,  WIDTH)))

		y = label

		yield X, y

```

Shuffling train data before iterating over dataset is good practice if the shuffling does not cost much. In our case, we just shuffle indices. So, it is quite easy:

```
indices = np.arange(len(train_images_path))
np.random.shuffle(indices)
```

**return** sends a specified value back to its caller whereas **yield** can produce a sequence of values. The yield statement suspends functionâ€™s execution and sends a value back to caller, but retains enough state to enable function to resume where it is left off. [See for more info: return vs yield](https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/) 
  

Next, we create the dataset object:

```
train_dataset = tf.data.Dataset.from_generator(generator = train_generator,
                                            output_types = (tf.float32, tf.int8),
                                            output_shapes=(tf.TensorShape([HEIGHT, WIDTH, 3]), tf.TensorShape([]))) ```
										

The ```from_generator``` creates ```tf.data.Dataset``` object for given sequences. While creating the object, we have to define **generator** and **output_types** arguments.  **generator** is simply the generator that we created, and **output_types** indicates the types of variables generated by the generator. **output_shapes** indicates the shapes of yielded parameters which is an image and a label in our case.



Then, we can use **train_dataset** as regular ```tf.data.Dataset```  object. For example, we can apply shuffling or preprocessing:

```
SHUFFLE_BUFFER_SIZE = BS_PER_GPU*5
train_dataset = train_dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).map(preprocess_train).batch(BS_PER_GPU, drop_remainder=True)
```

The ```tf.data.Dataset.shuffle()``` loads the data to the RAM to shuffle dataset. Although, it is good practice to load the whole dataset to the memory for shuffling. There might be errors due to lack of memory, especially when you have really large datasets. Therefore, it is good practice to write shuffling script in generator function. Then, you can select lower number of ```buffer_size```. In above example, we load the number of ```5*batch_sizes``` samples in to the memory for shuffling.


## Summary

This tutorial explained how to hande large datasets using generators and ```tf.data.Dataset```. 

Use this [repo](https://github.com/lambdal/TensorFlow2-tutorial/bozcani/master/06-train-on-large-dataset) to reproduce the results in this tutorial.
